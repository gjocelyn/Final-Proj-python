from bs4 import BeautifulSoup
import requests
import json
import time
# import secrets

API_KEY = '9Y8Kd_UGYAaiGegWJYQOWZ8chyPDsEwrLUe2UNjusDzKVZHXz76G1c4tRVFlEg9gl2-SRz-FYPVCi0_KuPAO-mi654JXSnvaFNCZxFTvMul5_ebuIV8fwkbEUPmTY3Yx'
CACHE_f = "yelp.json"
CACHE_dict = {}

YELP_URL_b = "https://api.yelp.com/v3/businesses/search?"
WIKI_URL = "https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population"
# API_KEY = secrets.API_KEY
headers = {'authorization': "Bearer " + API_KEY}

def read_json(filepath, encoding='utf-8'):
    """Reads a JSON document, decodes the file content, and returns a list or dictionary if
    provided with a valid filepath.

    Parameters:
    ----------
        filepath: str
            path to file
        encoding: str
            name of encoding used to decode the file

    Returns:
    ----------
        dict/list: dict or list representations of the decoded JSON document
    """

    with open(filepath, 'r', encoding=encoding) as file_obj:
        return json.load(file_obj)

def write_json(filepath, data, encoding='utf-8', ensure_ascii=False, indent=2):
    """Serializes object as JSON. Writes content to the provided filepath.

    Parameters:
    ----------
        filepath: str
            the path to the file
        data: dict/list
            the data to be encoded as JSON and written to the file
        encoding: str
            name of encoding used to encode the file
        ensure_ascii: str 
            if False non-ASCII characters are printed as is;
                            otherwise non-ASCII characters are escaped.
        indent: int
            number of "pretty printed" indention spaces applied to encoded JSON

    Returns:
    ----------
        None
    """
    # TODO
    # Uncomment the below lines and correct the mistakes

    with open(filepath, 'w', encoding=encoding) as file_obj:
        json.dump(data, file_obj, ensure_ascii=ensure_ascii, indent=indent)

def generate_idx(url, params):
    '''
    repeateable index of api url

    Parameters
    ----------
    url: string
        api url
    params: dict
        value pairs
    
    Returns
    ----------
    string
        repeateable index of api url
    '''
    param_list=[]
    for i in params.keys():
        param_list.append(f"{i}_{params[i]}")
    param_list.sort()
    print(param_list)
    uniq_idx = url + '+'.join(param_list)
    print(uniq_idx)
    return uniq_idx

def open_cache():
    ''' Open the cache file if exists and load JSON file into
    the CACHE_DICT dictionary or create a new cache dictionary
    
    Parameters
    ----------
    None
    
    Returns
    ----------
    The opened cache: dict
    '''
    try:
        cache_dict = read_json(CACHE_f)
    except:
        cache_dict = {}
    return cache_dict


def save_cache(cache_dict):
    ''' Save cache to disk
    
    Parameters
    ----------
    cache_dict: dict
    
    Returns
    ----------
    None
    '''
    write_json(CACHE_f, cache_dict)

def request_data(url, params):
    '''Make a request to the Web API
    
    Parameters
    ----------
    url: string
        The API URL
    params: dict
        value pairs
    
    Returns
    -------
    dict

    '''
    response = requests.get(url, params=params, headers=headers)
    return response.json()

def check_data(url, params):
    '''
    Check if cache has unique index (generated by generate_idx)
    If found, return it or send request to API url then save it.

    Parameters
    ----------
    url: string
        API url
    params: dictionary
        value pairs
    
    Returns
    ----------
    dict
        JSON file
    '''
    check_idx=generate_idx(url, params)
    if check_idx in CACHE_dict.keys():
        # print("It is stored in cache")
        return CACHE_dict[check_idx]
    else:
        # print("Request info to web and saving to cache")
        CACHE_dict[check_idx]=request_data(url, params)
        save_cache(CACHE_dict)
        return CACHE_dict[check_idx]

def city_info():
    
    '''
    scrapes the wikipedia page and save as a dictionary
    
    Parameters
    ----------
    none
    
    Returns
    ----------
    city_info: list
        a list of 326 different cities with the organized relevant information
    '''
    
    city_info = []
    url_text = requests.get(WIKI_URL).text
    soup = BeautifulSoup(url_text, 'html.parser')
    list = soup.find('table', class_='wikitable sortable').find('tbody').find_all('tr')[1:]
    for i in range(100):
        td_list = list[i].find_all('td')
        th_list = list[i].find_all('th')
        
        rank=int(th_list[0].text.strip())
        
        city=str(td_list[0].find('a').text.strip())
        
        try:
            state=str(td_list[1].find('a').text.strip())
        except:
            state=td_list[1].text.strip()
            
        population = int(td_list[2].text.strip().replace(',', ''))

        area = float(td_list[5].text.strip().split('\xa0')[0].replace(',', ''))
        time.sleep(0.1)
        city = {"rank": rank, "name":city, "state":state, "population": population, "area":area }
        city_info.append(city)
    
    return city_info

if __name__=='__main__':
    CACHE_dict = open_cache()
    # print(cache_dict.keys())
    location = input("Please enter the city you want to check:") # NYU
    url = YELP_URL_b + location
    param = {
        'location': location,
        'term': "food",
        'limit': 50
    }
    check_data(url, param)
    print(city_info())